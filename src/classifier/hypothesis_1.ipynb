{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Movie Review Classifier\n",
    "Author: Cody W. Eilar & Venktatesh Jatla<br/>\n",
    "Date: 10/9/15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read training negative reviews\n",
    "Do we want a dictionary or do we want to use pandas? I left the pandas code in there just in case it is useful in the end. If we end up not using it, let's delete it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_train_file = \"../../results/train_pos.txt\"\n",
    "neg_train_file = \"../../results/train_neg.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_array(file_name, sent_type): \n",
    "    \"\"\" \n",
    "    file_name = file to the data.\n",
    "    sent_type = sentiment type (+1 for positive -1 for negative review)\n",
    "    \"\"\"\n",
    "    data = pandas.read_csv(file_name, sep='\\t', quoting=csv.QUOTE_NONE, names=[\"word\", \"counts\"])\n",
    "    mat = np.ones(len(data))*sent_type; \n",
    "    data['type'] = mat\n",
    "    return data; \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neg_data = get_array(neg_train_file, -1)\n",
    "pos_data = get_array(pos_train_file, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we need to normalize the counts\n",
    "We are going to do this by dividing each word count by the total number of words in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def strip(text):\n",
    "    \"\"\"\n",
    "    Remove training whitespace\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return text.strip()\n",
    "    except AttributeError:\n",
    "        return text\n",
    "\n",
    "vocab_file = \"../../files/stemmed_vocab.txt\"\n",
    "vocab_data = pandas.read_csv(vocab_file, sep='\\n', quoting=csv.QUOTE_NONE, names=[\"vocab\"], converters={'vocab' : strip})\n",
    "#Somehow there are duplicates in this data. \n",
    "vocab_data = vocab_data.drop_duplicates()\n",
    "\n",
    "\n",
    "def normalize_freq(data_frame, vocab_data): \n",
    "    \"\"\"\n",
    "    Append a column with the frequency of occurence \n",
    "    \"\"\"\n",
    "    sum_count = data_frame[\"counts\"].sum()\n",
    "    data_frame['freq'] = data_frame[[\"counts\"]].divide(len(vocab_data))\n",
    "    return data_frame\n",
    "\n",
    "neg_data_norm = normalize_freq(neg_data, vocab_data)\n",
    "pos_data_norm = normalize_freq(pos_data, vocab_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##Limit the number of features\n",
    "We currently have a lot of features so we attempt to limit those by sorting the frequencies, and then only keeping the first 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neg_subset = neg_data_norm.sort(['counts'], ascending=0).head(5000)\n",
    "pos_subset = pos_data_norm.sort(['counts'], ascending=0).head(5000)\n",
    "print(\"*\"*50)\n",
    "print(\"The first 10 entries of the negative data\")\n",
    "print(\"*\"*50)\n",
    "print(neg_subset.head(10))\n",
    "print(\"\")\n",
    "print(\"*\"*50)\n",
    "print(\"The first 10 entries of the positive data\")\n",
    "print(\"*\"*50)\n",
    "print(pos_subset.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Combine the positive and negative data training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = neg_subset.append(pos_subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_vocab_idx(vocab_data, word): \n",
    "    return vocab_data[vocab_data[\"vocab\"] == word].index.tolist()[0]\n",
    "\n",
    "def create_vocab(vocab_data, data_frame): \n",
    "    \"\"\"\n",
    "    vocab_data = The data frame representing the vocabulary\n",
    "    data_frame = The data frame representing the training data\n",
    "    In this step we convert the words to their integer representation in the vocabulary.\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    for word in data_frame[\"word\"]: \n",
    "        indices.append(get_vocab_idx(vocab_data, word))\n",
    "    \n",
    "    data_frame['integer'] = indices; \n",
    "    return data_frame\n",
    "    \n",
    "print (vocab_data.head(10))\n",
    "\n",
    "new_train_data = create_vocab(vocab_data, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Display a few comparisons between words and the frequencies. \n",
    "We can see that for some entries that the frequencies are pretty close!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(new_train_data.sort('integer', ascending=1).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(new_train_data[['integer', 'counts']].values, y=new_train_data[['type']].values)\n",
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier( penalty='l2', alpha=1e-3, n_iter=5, random_state=42)\n",
    "Y = new_train_data[['type']].values\n",
    "clf.fit(tfidf, Y.ravel())\n",
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Check accuracy\n",
    "To see if this is a good method for classification, we now need to create a confusion matrix of the predicted lables vs the actual labels. To do this with the method we have chosen, we simply sum the predicted labels. If the sum is less than 0 than we classify it as a negative review and if is postive then we classify it as a positive review. I've taken all the steps above and codensed them into vectorize.py to make the transformation of text to matrix more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from vectorizer import vectorizer\n",
    "import glob\n",
    "files_dir = [\"/Users/cody/Downloads/aclImdb/train/pos/*.txt\", \"/Users/cody/Downloads/aclImdb/train/neg/*.txt\"]            \n",
    "path_to_mapper = \"/Users/cody/Repos/SentimentAnalysis/build/bin/mapper\"      \n",
    "path_to_reducer = \"/Users/cody/Repos/SentimentAnalysis/build/bin/reducer.py\" \n",
    "path_to_vocab = \"/Users/cody/Repos/SentimentAnalysis/files/stemmed_vocab.txt\"\n",
    "\n",
    "predicted_y = []\n",
    "actual_y = []\n",
    "for types in files_dir:\n",
    "    iterations = 0\n",
    "    for file_name in glob.iglob(types): \n",
    "        if iterations < 20: \n",
    "            test = vectorizer(file_name, path_to_mapper, path_to_reducer, path_to_vocab)\n",
    "            a = clf.predict(test[0])\n",
    "            if a.sum() > 0: \n",
    "                predicted_y.append(1)\n",
    "            else: \n",
    "                predicted_y.append(-1); \n",
    "\n",
    "            if test[1].sum() > 0: \n",
    "                actual_y.append(1)\n",
    "            else: \n",
    "                actual_y.append(-1)\n",
    "        else: \n",
    "            break\n",
    "        iterations = iterations + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "cm = confusion_matrix(actual_y, predicted_y)\n",
    "%matplotlib inline\n",
    "print(cm)\n",
    "plt.matshow(cm)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "We can see from the above plot that we are not doing a very good job of classifying. Something is not working very well. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
